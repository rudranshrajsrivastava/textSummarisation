# tokenisation and word split


tokens=[token.text.lower() for token in doc if not token.is_stop and not token.is_punct and token.text !='\n']
tokens

tokens1=[]
stopwords=list(STOP_WORDS)
allowed_pos=['ADJ','PROPN','VERB','NOUN']
for token in doc:
    if token.text in stopwords or token.text in punctuation:
        continue
        if token.pos_ in allowed_pos:
          tokens1.append(token.text)

from collections import Counter
word_freq=Counter(tokens)
word_freq
